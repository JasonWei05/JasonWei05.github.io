---
layout: post
title: "Tongyi DeepResearch Technical Report"
date: 2025-11-30
tags: [Agent]
paper_url: ""
---

* Table of Contents
{:toc}

# Abstract

We present Tongyi DeepResearch, an agentic large language model, which
is specifically designed for long-horizon, deep information-seeking research
tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch
is developed through an end-to-end training framework that combines agentic
mid-training and agentic post-training, enabling scalable reasoning and
information seeking across complex tasks. We design a highly scalable data
synthesis pipeline that is fully automatic, without relying on costly human
annotation, and empowers all training stages. By constructing customized
environments for each stage, our system enables stable and consistent interactions
throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters,
with only 3.3 billion activated per token, achieves state-of-the-art performance
across a range of agentic deep research benchmarks, including Humanityâ€™s Last
Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch,
FRAMES and xbench-DeepSearch-2510. We open-source the model, framework,
and complete solutions to empower the community.

# Takeaways
- Tongyi DR begins training from the Qwen3-30B-A3B Base model. They claim that general foundation models usually loack agentic inductive bias, which points them to starting with mid-training from a base model. They use mid-training to endow the pre-trained base model with agentic prior knowledge. 
- It follows the vanilla ReAct framework.
- A context mangaement paradigm is introduced based on Markovian state reconstruction, where the agent is not conditioned on the complete history. The agent is conditioned on the question, an evolving report serving as compressed memory, and the previous interaction (both action and observation but not thought). This prevents context suffocation and aligns with human research patterns. 
- For mid-training, two stages of Agentic Continual Pre-training are used. The first with 32K context length and the second with 128K context length. 
- Post-training is also done in two stages: SFT and Agentic RL
    - SFT data comes from high-performance open-source models after being filtered through rejection sampling. There are two stages. The first is at 40K context length, where both ReAct model and Context Management Mode samples are used, and the second stage is at 128K context length, where the training data is mostly in ReAct mode and a small portion of 40K data is sprinkled in.
    - During agentic RL, the agent toolkit consists of (1) search, (2) visit, (3) python interpreter, (4) Google Scholar, (5) file parser. For each tool, they pay special important to robust concurrency, fault-tolerance mechanisms, results caching, timeout and retry protocols, etc. Rollouts are done asynchronously. The RL algorithm is based on GRPO with a few modifications: token-level policy gradient loss (DAPO), clip-higher (DAPO), leave-one-out (like RLOO from Chen et al., 2025).

> "The success of agentic RL depends more on
the quality of the data and the stability of the training environment than on the specific algorithm being used."

- Model merging is the technique of computing a weighted average of multiple models to get a final merged model. They find that the merged model performs comparably to the best-perfoming model. 
- Evaluated on benchmarks, including Humanity's Last Exam, Browse Comp, GAIA, xbench DS, WebWalker QA, and Frames, Tongyi DR beats out or follows closely behind many open and closed-source models with trillions of parameters, including OpenAI DR, Gemini DR, Kimi Researcher, and GLM 4.5.

# Thoughts
They don't seem to release any data or their data synthesis pipeline, which is sad. It is still quite impressive that they can beat a lot of the top closed-source models with just a mid/post-trained Qwen3-30B-A3B Base model. The mid-training seems to be their special sauce that allows the model to be natively agentic. If you want an LLM to be good at very at one task, a large portion of your data should be focus on that one task.