---
layout: post
title: "ReAct: Synergizing Reasoning and Acting in Language Models. Yao et al."
date: 2025-11-30
tags: [Agent]
paper_url: ""
---


# Abstract

While large language models (LLMs) have demonstrated impressive performance
across tasks in language understanding and interactive decision making, their
abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action
plan generation) have primarily been studied as separate topics. In this paper, we
explore the use of LLMs to generate both reasoning traces and task-specific actions
in an interleaved manner, allowing for greater synergy between the two: reasoning
traces help the model induce, track, and update action plans as well as handle
exceptions, while actions allow it to interface with and gather additional information
from external sources such as knowledge bases or environments. We apply our
approach, named ReAct, to a diverse set of language and decision making tasks
and demonstrate its effectiveness over state-of-the-art baselines in addition to
improved human interpretability and trustworthiness. Concretely, on question
answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent
issues of hallucination and error propagation in chain-of-thought reasoning by
interacting with a simple Wikipedia API, and generating human-like task-solving
trajectories that are more interpretable than baselines without reasoning traces.
Furthermore, on two interactive decision making benchmarks (ALFWorld and
WebShop), ReAct outperforms imitation and reinforcement learning methods by
an absolute success rate of 34% and 10% respectively, while being prompted with
only one or two in-context examples.

# Takeways
- In a generic setup of an agent alternating between taking actions and receiving observations, ReAct augments the agent's action space by allowing the agent to think in the language space before outputting an action. This thinking space allows the agent to reason over the current context.
- The authors claim that the ReAct framework is intuitive and easy to design, general and flexible, performant and robust, and human-aligned and controllable. 
- ReAct performs very well on benchmarks.

# Thoughts
ReAct seems to be the major paridgm used even today to build agents. The biggest competitor is interleaved reasoning, but ReAct has the advantage of being easier to train and monitor. Interleaved reasoning would be harder to enforce and would probably work better with a process reward model. 
