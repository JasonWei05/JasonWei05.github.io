<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://jasonwei05.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://jasonwei05.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-27T00:44:24+00:00</updated><id>https://jasonwei05.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Policy Gradient, TRPO, PPO, DPO</title><link href="https://jasonwei05.github.io/blog/2024/policy-grad-methods/" rel="alternate" type="text/html" title="Policy Gradient, TRPO, PPO, DPO"/><published>2024-12-24T14:14:00+00:00</published><updated>2024-12-24T14:14:00+00:00</updated><id>https://jasonwei05.github.io/blog/2024/policy-grad-methods</id><content type="html" xml:base="https://jasonwei05.github.io/blog/2024/policy-grad-methods/"><![CDATA[<h2 id="prereqs">Prereqs</h2> <p>We assume prerequisite knowledge in Reinforcement Learning definitions (MDPs, etc.), some Calculus, and some Linear Algebra.</p> <h2 id="policy-gradient">Policy Gradient</h2> <p>Let $\theta$ be the parameters for a policy $\pi_\theta$. Consider a trajectory $\tau = s_1,a_1,s_2,a_2,…,s_T,a_T$ and the trajectory distribution</p> \[p_\theta(s_1,a_1,s_2,a_2,...,s_T,a_T) = p(s_1) \prod_{t=1}^T\pi_\theta(a_t|s_t) p(s_{t+1}|s_t,a_t).\] <table> <tbody> <tr> <td>We can think this as probability of taking the trajectory $\tau$ under the policy $\pi_\theta$ and the transition function $p(s_{t+1}</td> <td>s_t,a_t)$. On the RHS, $p(s_1)$ is the probability begin the trajectory at $s_1$, $\pi_\theta(a_t</td> <td>s_t)$ is the probability that our policy takes action $a_t$ in state $s_t$, and $p(s_{t+1}</td> <td>s_t,a_t)$ is the probability the enviornemnt transitions to $s_{t+1}$ when in state $s_t$ and taking action $a_t.$</td> </tr> </tbody> </table> <p>Let $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ be a reward function that takes in a state and a action and ouputs the reward. The goal in RL is to maximize reward, so we are looking for the optimal parameter $\theta^*$ such that</p> \[\theta^* = \text{argmax}_\theta \mathbb{E}_{(s,a)\sim p_\theta(s,a)}\left[\sum_{t=1}^T r(s_t,a_t)\right].\] <p>Let $J(\theta) = \mathbb{E}<em>{(s,a)\sim p</em>\theta(s,a)}\left[\sum_{t=1}^T r(s_t,a_t)\right]$ be our objective function that we want to maximize. To make the notation cleaner, we introduce the return function $R$ so that $R(\tau) = \sum_{t=1}^T r(s_t,a_t)$. Then</p> \[J(\theta) = \int p_\theta(\tau) R(\tau) d\tau.\] <p>To optimize $J(\theta)$, we would like to use a first-order (gradient) method, so let’s calculate $\nabla_\theta J(\theta).$</p> <p>\(\begin{aligned} \nabla_\theta J(\theta) &amp; = \int \nabla_\theta p_\theta(\tau) R(\tau) d\tau\\ &amp; = \int p_\theta(\tau) \frac{\nabla_\theta p_\theta(\tau)}{p_\theta(\tau)} R(\tau) d\tau \quad \text{(multiply and divide by $p_\theta(\tau)$)}\\ &amp; = \int p_\theta(\tau) \nabla_\theta \log (p_\theta(\tau)) R(\tau) d\tau \quad \text{(by derivative of log)} \\ &amp; = \mathbb{E}_{\tau \sim p_\theta(\tau)}[\nabla_\theta \log (p_\theta(\tau)) R(\tau)] \end{aligned}\) Let’s now expand $\nabla_\theta \log p_\theta(\tau)$.</p> \[\begin{aligned} \nabla_\theta \log p_\theta(\tau) &amp; = \nabla_\theta (\log p(s_1) + \sum_{t=1}^T (\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t,a_t))) \quad \text{(by log rule)}\\ &amp; = \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \quad \text{(only terms that are function of $\theta$)} \end{aligned}\] <p>Substituting this into $J(\theta)$, we have</p> \[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \left( \sum_{t=1}^T \nabla \log\pi_\theta (a_t|s_t) \right) \left(\sum_{t=1}^T r(s_t,a_t)\right) \right],\] <p>We can estimate this value by sampling $N$ trajectories. Then we would have \(\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=1}^T \nabla \log\pi_\theta (a_{i,t}|s_{i,t}) \right) \left(\sum_{t=1}^T r(s_{i,t},a_{i,t})\right).\)</p> <p>With the estimate of $\nabla_\theta J(\theta),$ we can gradient ascent by \(\theta \leftarrow \theta + \alpha \nabla J(\theta)\) with step size $\alpha$. This is the REINFORCE algorithm (Williams 1992). Sample trajectories, estimate the gradient using the trajectories, update $\theta$.</p> <p>This algorithm will work in theory, but there are many improvements that we can make to reduce variance.</p> <h3 id="rewards-to-go">Rewards To Go</h3> <p>We can reformulate $\nabla_\theta J(\theta)$ as</p> \[\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla \log\pi_\theta (a_{i,t}|s_{i,t}) \left(\sum_{t'=1}^T r(s_{i,t'},a_{i,t'})\right).\] <p>Notice that at time step $t$, we are still summing over all rewards from time steps $t’ &lt; t.$ These rewards should not</p> <h3 id="trust-region-policy-optimization">Trust Region Policy Optimization</h3> <h3 data-toc-text="Customizing" id="proximal-policy-optimization">Proximal Policy Optimization</h3> <h3 id="direct-preference-optimization">Direct Preference Optimization</h3>]]></content><author><name></name></author><category term="Machine"/><category term="Learning"/><category term="RL,"/><category term="LLM"/><summary type="html"><![CDATA[Things I learned recently about advanced Policy Gradient Methods.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://jasonwei05.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://jasonwei05.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://jasonwei05.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://jasonwei05.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://jasonwei05.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://jasonwei05.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>